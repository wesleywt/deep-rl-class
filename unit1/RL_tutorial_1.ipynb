{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training for Reinforcement Learning of a Lunar Lander\n",
    "This is the first tutorial of HuggingFaces Deep Reinforcement Learning. I would like to run everything on my PC, but have been struggling with the installation. The problem seems to be that the Python versions in my installations might be too new for some of the RL libraries.\n",
    "\n",
    "The following is the solution to these installation problems\n",
    "\n",
    "## Installing libraries in WSL2\n",
    "WSL2 is now able to run Linux GUIs without the need for other X-term emulators.\n",
    "### Install Ananconda and python-opengl\n",
    "The libraries require python-opengl, xvfb. I am installing the rest because it works for now.\n",
    "1. Install Anaconda3 on WSL2\n",
    "2. ```sudo apt-get install python-opengl```\n",
    "3. ```sudo apt-get install cmake zlib1g-dev xorg-dev libgtk2.0-0 swig python-opengl xvfb``` - note that ```python-matplotlib``` is not installed\n",
    "\n",
    "### Create the environment\n",
    "Initially I created the conda environment with python=3.5. But the latest version python=3.9.12 seems to work just fine.\n",
    "\n",
    "Don't use python=3.10\n",
    "\n",
    "But you can set the version using\n",
    "```\n",
    "conda create --name gym python=3.5\n",
    "```\n",
    "For the conda python version\n",
    "```\n",
    "conda create --name gym\n",
    "```\n",
    "Activate the environment\n",
    "\n",
    "```\n",
    "source activate gym\n",
    "```\n",
    "Now install the OpenAI Gym package\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "\n",
    "pip install gym[atari]\n",
    "\n",
    "pip install gym[box2d]\n",
    "\n",
    "pip install box2d-py\n",
    "\n",
    "pip install box2d\n",
    "\n",
    "pip install box2d-kengz\n",
    "```\n",
    "I have been having huge issues with gym[box2d], but now its installing just fine with this method. I couldn't run the PacMan because I don't have a license.\n",
    "\n",
    "I am sure this method will work for my Linux problems too. But I am having issues with the CUDA installation after the update to 22.04. I might need to install 20.04 again to solve these.\n",
    "\n",
    "\n",
    "### Install Python Virtual Display\n",
    "The next problem was with the virtual display.\n",
    "\n",
    "\n",
    "```pip install pyvirtualdisplay```\n",
    "\n",
    "I used Pycharm to install this\n",
    "\n",
    "The final part that kills WSL is that Jupyter doesn't work.\n",
    "\n",
    "### Install Pytorch\n",
    "Installed Pytorch using the pip command from Pytorch.org\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyvirtualdisplay.display.Display at 0x7f089c1d1910>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing Huggingface Libraries\n",
    "```\n",
    "pip install huggingface-sb\n",
    "\n",
    "pip install huggingface-hub\n",
    "\n",
    "pip install stable-baselines3\n",
    "\n",
    "```\n",
    "\n",
    "## What is GYM and how it works\n",
    "The library containing our environment is called Gym. Gym is used a lot in Deep Reinforcement Learning.\n",
    "\n",
    "The GYM library provides two things:\n",
    "* An interface that allows you to create RL environments\n",
    "* A collection of environments (gym-control, atari, box2d)\n",
    "\n",
    "### The reinforcement learning loop\n",
    "A recap on the RL loop:\n",
    "\n",
    "1. The agent receives state S0 from the Environment - The first frame of the game\n",
    "2. The agent takes action A0 - The agent makes a move to the right\n",
    "3. The environment creates a new state S1 - A new frame from the game\n",
    "4. The environment gives a reward R1 to the Agent- If not dead Positive Reward +1\n",
    "\n",
    "### The RL loop in Gym\n",
    "1. The environment is created by ```gym.make()```\n",
    "2. Reset the environment to its initial state with ```observation = env.reset()```\n",
    "3. Using ```env.step(action)``` we perform an action in the environment (a random action) and we receive.\n",
    "            * ```observation```: The new state S1\n",
    "            * ```reward```: Reward for the action\n",
    "            * ```done```: Indicates if the episode terminated\n",
    "            * ```info```: A dictionary that provides additional information (depends on the environment)\n",
    "\n",
    "If the episode is done, we reset the environment to its initial sates with ```observation = env.reset()```."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken 1\n",
      "Action taken 0\n",
      "Action taken 3\n",
      "Action taken 3\n",
      "Action taken 1\n",
      "Action taken 3\n",
      "Action taken 3\n",
      "Action taken 0\n",
      "Action taken 3\n",
      "Action taken 2\n",
      "Action taken 0\n",
      "Action taken 2\n",
      "Action taken 3\n",
      "Action taken 3\n",
      "Action taken 2\n",
      "Action taken 3\n",
      "Action taken 0\n",
      "Action taken 1\n",
      "Action taken 2\n",
      "Action taken 2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the environment called LunarLander V2 from box2d\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Reset the environment to S0\n",
    "observation = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    print(f\"Action taken {action}\")\n",
    "\n",
    "    # Perform the action and receive the next_state, reward, done and info\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Environment is reset\") # reset the environment\n",
    "        observation = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the LunarLander Environment and understanding how it works\n",
    "We are going to train a Lunar Lander to land correctly on the moon. We need the agent to learn to adapt its speed and position (horizontal, vertical and angular) to land correctly.\n",
    "\n",
    "### Lunar Lander Documentation\n",
    "[lunar_lander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Observation Space-------------------\n",
      "\n",
      "Observation Space Shape (8,)\n",
      "Sample observation [ 1.2334801  -1.1262255   0.59130734 -0.23976912  0.32084632  0.44667673\n",
      "  1.2959138  -0.4295383 ]\n"
     ]
    }
   ],
   "source": [
    "# create the environment with gym.make()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"----------------Observation Space-------------------\\n\")\n",
    "print(f'Observation Space Shape {env.observation_space.shape}')\n",
    "print(f'Sample observation {env.observation_space.sample()}') # get a random observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the Observation shape ```(8,)``` that the observation is vector of size 8 ,each value is information about the lander.\n",
    "\n",
    "1. Horizon pad coordinate (x)\n",
    "2. Vertical pad coordinate (y)\n",
    "3. Horizontal speed (x)\n",
    "4. Vertical speed (y)\n",
    "5. Angle\n",
    "6. Angular speed\n",
    "7. If the left leg has contact point touches the land\n",
    "8. If the right leg has contact point touched the land"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________________Action Space_______________\n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n___________________Action Space_______________\\n\")\n",
    "print(f\"Action Space Shape {env.action_space.n}\")\n",
    "print(f'Action Space Sample {env.action_space.sample()}') # takes a random action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The action space is the set of possible actions the agent can make. It is discrete with 4 actions available:\n",
    "1. Do nothing\n",
    "2. Fire left orientation engine\n",
    "3. Fire the main engine\n",
    "4. Fire right orientation engine\n",
    "\n",
    "The Reward Function is the function that will give a reward at each time step:\n",
    "1. Moving from the top of the screen to the landing pad and zero speed is ~ 100 to 140 points\n",
    "2. Firing main engine is -0.3 each frame\n",
    "3. Each leg ground contact is + 10 points\n",
    "4. If episode finishes with a crash -100 points or comes to rest + 100 points\n",
    "5. The game is solved if the agent has 200 points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorize the Environment\n",
    "A vectorized environment is a way to stack multiple independent environments into a single environment.\n",
    "We create a vectorized environment of 16 environments, so that we will have a more diverse experience during training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import  make_vec_env\n",
    "# create the environment with 16 independent environment scenarios\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the Model\n",
    "We have created an environment that enables the Lunar Lander to land correctly on a Landing Pad by controlling left, right and main orientation engine.\n",
    "\n",
    "We need to now build the algorithm that will solve the problem.\n",
    "\n",
    "We use the Deep RL library Stable Baselines 3 (SB#) to do this\n",
    "SB2 is a set of reliable implementations of reinforcement learning algorithms in Pytorch\n",
    "\n",
    "### Stable Baseline 3\n",
    "[documentation and tutorials](https://stable-baselines3.readthedocs.io/en/master/)\n",
    "\n",
    "### Solving the problem with SB3\n",
    "We are going to use SB3 PPO. PPO (Proximal Policy Optimization) is not of the state-of-the-art Deep Reinforcement Learning algorithms that will be studied in this course.\n",
    "\n",
    "PPO is a combination of:\n",
    "* Value-based reinforcement learning: learning an action-value function that will tell us what is the most valuable action to take given a state and action\n",
    "* Policy based reinforcement learning: learning a policy that will give us a probability distribution over actions.\n",
    "\n",
    "### Setting up SB3\n",
    "1. Create the environment\n",
    "2. Define the model you want to use and instantiate the model with ```model = PPO('MlpPolicy')```\n",
    "3. Train the agent with ```model.learn``` and define the number of training time steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Define a PPO MlpPolicy architecture\n",
    "# MultilayerPerceptron Policy\n",
    "# We use the MlpPolicy because we are using as input a vector\n",
    "# If we use frames as input we will use CnnPolicy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "model  = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}