{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training for Reinforcement Learning of a Lunar Lander\n",
    "This is the first tutorial of HuggingFaces Deep Reinforcement Learning. I would like to run everything on my PC, but have been struggling with the installation. The problem seems to be that the Python versions in my installations might be too new for some of the RL libraries.\n",
    "\n",
    "The following is the solution to these installation problems\n",
    "\n",
    "## Installing libraries in WSL2\n",
    "WSL2 is now able to run Linux GUIs without the need for other X-term emulators.\n",
    "### Install Ananconda and python-opengl\n",
    "The libraries require python-opengl, xvfb. I am installing the rest because it works for now.\n",
    "1. Install Anaconda3 on WSL2\n",
    "2. ```sudo apt-get install python-opengl```\n",
    "3. ```sudo apt-get install cmake zlib1g-dev xorg-dev libgtk2.0-0 swig python-opengl xvfb``` - note that ```python-matplotlib``` is not installed\n",
    "\n",
    "### Create the environment\n",
    "Initially I created the conda environment with python=3.5. But the latest version python=3.9.12 seems to work just fine.\n",
    "\n",
    "Don't use python=3.10\n",
    "\n",
    "But you can set the version using\n",
    "```\n",
    "conda create --name gym python=3.5\n",
    "```\n",
    "For the conda python version\n",
    "```\n",
    "conda create --name gym\n",
    "```\n",
    "Activate the environment\n",
    "\n",
    "```\n",
    "source activate gym\n",
    "```\n",
    "Now install the OpenAI Gym package\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "\n",
    "pip install gym[atari]\n",
    "\n",
    "pip install gym[box2d]\n",
    "\n",
    "pip install box2d-py\n",
    "\n",
    "pip install box2d\n",
    "\n",
    "pip install box2d-kengz\n",
    "```\n",
    "I have been having huge issues with gym[box2d], but now its installing just fine with this method. I couldn't run the PacMan because I don't have a license.\n",
    "\n",
    "I am sure this method will work for my Linux problems too. But I am having issues with the CUDA installation after the update to 22.04. I might need to install 20.04 again to solve these.\n",
    "\n",
    "\n",
    "### Install Python Virtual Display\n",
    "The next problem was with the virtual display.\n",
    "\n",
    "\n",
    "```pip install pyvirtualdisplay```\n",
    "\n",
    "I used Pycharm to install this\n",
    "\n",
    "The final part that kills WSL is that Jupyter doesn't work.\n",
    "\n",
    "### Install Pytorch\n",
    "Installed Pytorch using the pip command from Pytorch.org\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyvirtualdisplay.display.Display at 0x7fee30a37d60>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=True, size=(1400, 900))\n",
    "virtual_display.start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing Huggingface Libraries\n",
    "```\n",
    "pip install huggingface-sb\n",
    "\n",
    "pip install huggingface-hub\n",
    "\n",
    "pip install stable-baselines3\n",
    "\n",
    "```\n",
    "\n",
    "## What is GYM and how it works\n",
    "The library containing our environment is called Gym. Gym is used a lot in Deep Reinforcement Learning.\n",
    "\n",
    "The GYM library provides two things:\n",
    "* An interface that allows you to create RL environments\n",
    "* A collection of environments (gym-control, atari, box2d)\n",
    "\n",
    "### The reinforcement learning loop\n",
    "A recap on the RL loop:\n",
    "\n",
    "1. The agent receives state S0 from the Environment - The first frame of the game\n",
    "2. The agent takes action A0 - The agent makes a move to the right\n",
    "3. The environment creates a new state S1 - A new frame from the game\n",
    "4. The environment gives a reward R1 to the Agent- If not dead Positive Reward +1\n",
    "\n",
    "### The RL loop in Gym\n",
    "1. The environment is created by ```gym.make()```\n",
    "2. Reset the environment to its initial state with ```observation = env.reset()```\n",
    "3. Using ```env.step(action)``` we perform an action in the environment (a random action) and we receive.\n",
    "            * ```observation```: The new state S1\n",
    "            * ```reward```: Reward for the action\n",
    "            * ```done```: Indicates if the episode terminated\n",
    "            * ```info```: A dictionary that provides additional information (depends on the environment)\n",
    "\n",
    "If the episode is done, we reset the environment to its initial sates with ```observation = env.reset()```."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken 3\n",
      "Action taken 1\n",
      "Action taken 1\n",
      "Action taken 1\n",
      "Action taken 3\n",
      "Action taken 2\n",
      "Action taken 0\n",
      "Action taken 1\n",
      "Action taken 0\n",
      "Action taken 2\n",
      "Action taken 0\n",
      "Action taken 0\n",
      "Action taken 1\n",
      "Action taken 0\n",
      "Action taken 3\n",
      "Action taken 2\n",
      "Action taken 3\n",
      "Action taken 2\n",
      "Action taken 2\n",
      "Action taken 0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the environment called LunarLander V2 from box2d\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Reset the environment to S0\n",
    "observation = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    print(f\"Action taken {action}\")\n",
    "\n",
    "    # Perform the action and receive the next_state, reward, done and info\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Environment is reset\") # reset the environment\n",
    "        observation = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the LunarLander Environment and understanding how it works\n",
    "We are going to train a Lunar Lander to land correctly on the moon. We need the agent to learn to adapt its speed and position (horizontal, vertical and angular) to land correctly.\n",
    "\n",
    "### Lunar Lander Documentation\n",
    "[lunar_lander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Observation Space-------------------\n",
      "\n",
      "Observation Space Shape (8,)\n",
      "Sample observation [-0.86316216  0.08773206 -0.4644909  -0.90330577 -0.5794051  -0.6585583\n",
      "  1.1527103   0.08829654]\n"
     ]
    }
   ],
   "source": [
    "# create the environment with gym.make()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"----------------Observation Space-------------------\\n\")\n",
    "print(f'Observation Space Shape {env.observation_space.shape}')\n",
    "print(f'Sample observation {env.observation_space.sample()}') # get a random observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the Observation shape ```(8,)``` that the observation is vector of size 8 ,each value is information about the lander.\n",
    "\n",
    "1. Horizon pad coordinate (x)\n",
    "2. Vertical pad coordinate (y)\n",
    "3. Horizontal speed (x)\n",
    "4. Vertical speed (y)\n",
    "5. Angle\n",
    "6. Angular speed\n",
    "7. If the left leg has contact point touches the land\n",
    "8. If the right leg has contact point touched the land"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________________Action Space_______________\n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n___________________Action Space_______________\\n\")\n",
    "print(f\"Action Space Shape {env.action_space.n}\")\n",
    "print(f'Action Space Sample {env.action_space.sample()}') # takes a random action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The action space is the set of possible actions the agent can make. It is discrete with 4 actions available:\n",
    "1. Do nothing\n",
    "2. Fire left orientation engine\n",
    "3. Fire the main engine\n",
    "4. Fire right orientation engine\n",
    "\n",
    "The Reward Function is the function that will give a reward at each time step:\n",
    "1. Moving from the top of the screen to the landing pad and zero speed is ~ 100 to 140 points\n",
    "2. Firing main engine is -0.3 each frame\n",
    "3. Each leg ground contact is + 10 points\n",
    "4. If episode finishes with a crash -100 points or comes to rest + 100 points\n",
    "5. The game is solved if the agent has 200 points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorize the Environment\n",
    "A vectorized environment is a way to stack multiple independent environments into a single environment.\n",
    "We create a vectorized environment of 16 environments, so that we will have a more diverse experience during training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import  make_vec_env\n",
    "# create the environment with 16 independent environment scenarios\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the Model\n",
    "We have created an environment that enables the Lunar Lander to land correctly on a Landing Pad by controlling left, right and main orientation engine.\n",
    "\n",
    "We need to now build the algorithm that will solve the problem.\n",
    "\n",
    "We use the Deep RL library Stable Baselines 3 (SB#) to do this\n",
    "SB2 is a set of reliable implementations of reinforcement learning algorithms in Pytorch\n",
    "\n",
    "### Stable Baseline 3\n",
    "[documentation and tutorials](https://stable-baselines3.readthedocs.io/en/master/)\n",
    "\n",
    "### Solving the problem with SB3\n",
    "We are going to use SB3 PPO. PPO (Proximal Policy Optimization) is not of the state-of-the-art Deep Reinforcement Learning algorithms that will be studied in this course.\n",
    "\n",
    "PPO is a combination of:\n",
    "* Value-based reinforcement learning: learning an action-value function that will tell us what is the most valuable action to take given a state and action\n",
    "* Policy based reinforcement learning: learning a policy that will give us a probability distribution over actions.\n",
    "\n",
    "### Setting up SB3\n",
    "1. Create the environment\n",
    "2. Define the model you want to use and instantiate the model with ```model = PPO('MlpPolicy')```\n",
    "3. Train the agent with ```model.learn``` and define the number of training time steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define a PPO MlpPolicy architecture\n",
    "# MultilayerPerceptron Policy\n",
    "# We use the MlpPolicy because we are using as input a vector\n",
    "# If we use frames as input we will use CnnPolicy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "model  = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the PPO agent\n",
    "* Train the agent for 500 000 time steps.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 289      |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    fps             | 3663     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 285          |\n",
      "|    ep_rew_mean          | 269          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2633         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050933086 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.781       |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.32         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | 4.88e-05     |\n",
      "|    value_loss           | 92.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 280          |\n",
      "|    ep_rew_mean          | 267          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2459         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035924343 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.749       |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.73         |\n",
      "|    n_updates            | 284          |\n",
      "|    policy_gradient_loss | -0.0004      |\n",
      "|    value_loss           | 145          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 273         |\n",
      "|    ep_rew_mean          | 266         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2377        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004420058 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.59        |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | -0.000519   |\n",
      "|    value_loss           | 71.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 276          |\n",
      "|    ep_rew_mean          | 263          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2308         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034710579 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.754       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.6         |\n",
      "|    n_updates            | 292          |\n",
      "|    policy_gradient_loss | 1.36e-05     |\n",
      "|    value_loss           | 281          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 274          |\n",
      "|    ep_rew_mean          | 264          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2253         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039390125 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.738       |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.63         |\n",
      "|    n_updates            | 296          |\n",
      "|    policy_gradient_loss | 0.000647     |\n",
      "|    value_loss           | 11.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 278          |\n",
      "|    ep_rew_mean          | 267          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2227         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045672227 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.711       |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.8         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 217          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 275          |\n",
      "|    ep_rew_mean          | 276          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2206         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037092424 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.716       |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 304          |\n",
      "|    policy_gradient_loss | 0.000124     |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 264          |\n",
      "|    ep_rew_mean          | 272          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2184         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034138956 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.96         |\n",
      "|    n_updates            | 308          |\n",
      "|    policy_gradient_loss | -0.000226    |\n",
      "|    value_loss           | 11.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 268          |\n",
      "|    ep_rew_mean          | 266          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2171         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030241208 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.36         |\n",
      "|    n_updates            | 312          |\n",
      "|    policy_gradient_loss | -0.000149    |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 264         |\n",
      "|    ep_rew_mean          | 270         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2160        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003775626 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.721      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 316         |\n",
      "|    policy_gradient_loss | -0.000784   |\n",
      "|    value_loss           | 208         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 266        |\n",
      "|    ep_rew_mean          | 273        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2150       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 196608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00391626 |\n",
      "|    clip_fraction        | 0.0443     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.708     |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.29       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | 0.0006     |\n",
      "|    value_loss           | 76.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 260          |\n",
      "|    ep_rew_mean          | 274          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2146         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044936603 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.712       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 324          |\n",
      "|    policy_gradient_loss | 0.000337     |\n",
      "|    value_loss           | 4.89         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 255         |\n",
      "|    ep_rew_mean          | 277         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2143        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004384498 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 328         |\n",
      "|    policy_gradient_loss | -0.000335   |\n",
      "|    value_loss           | 5.21        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | 277          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2149         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039032875 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.707       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.48         |\n",
      "|    n_updates            | 332          |\n",
      "|    policy_gradient_loss | -0.000521    |\n",
      "|    value_loss           | 3.38         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 252          |\n",
      "|    ep_rew_mean          | 273          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2146         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044706264 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.722       |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.37         |\n",
      "|    n_updates            | 336          |\n",
      "|    policy_gradient_loss | 9.96e-05     |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 255         |\n",
      "|    ep_rew_mean          | 276         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2146        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004042231 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.718      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000825   |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 253          |\n",
      "|    ep_rew_mean          | 277          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2144         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041960496 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.705       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 344          |\n",
      "|    policy_gradient_loss | -0.000302    |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 265          |\n",
      "|    ep_rew_mean          | 267          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2139         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037378715 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.697       |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 348          |\n",
      "|    policy_gradient_loss | -0.000352    |\n",
      "|    value_loss           | 74.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 266          |\n",
      "|    ep_rew_mean          | 272          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2138         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 153          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042592147 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.689       |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37         |\n",
      "|    n_updates            | 352          |\n",
      "|    policy_gradient_loss | -0.000724    |\n",
      "|    value_loss           | 139          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 249         |\n",
      "|    ep_rew_mean          | 275         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2143        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004711969 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.8        |\n",
      "|    n_updates            | 356         |\n",
      "|    policy_gradient_loss | -0.00013    |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 245         |\n",
      "|    ep_rew_mean          | 271         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2145        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004394302 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.000562    |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 244          |\n",
      "|    ep_rew_mean          | 273          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2144         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 376832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038925563 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.676       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.729        |\n",
      "|    n_updates            | 364          |\n",
      "|    policy_gradient_loss | 0.00101      |\n",
      "|    value_loss           | 3.3          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 241         |\n",
      "|    ep_rew_mean          | 271         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2144        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004556524 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 368         |\n",
      "|    policy_gradient_loss | -0.000935   |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 242         |\n",
      "|    ep_rew_mean          | 269         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2147        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003030045 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.5        |\n",
      "|    n_updates            | 372         |\n",
      "|    policy_gradient_loss | -1.98e-05   |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 239         |\n",
      "|    ep_rew_mean          | 276         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2153        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004279875 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 233         |\n",
      "|    n_updates            | 376         |\n",
      "|    policy_gradient_loss | 3e-05       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 244         |\n",
      "|    ep_rew_mean          | 272         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2153        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005568732 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.2        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 0.000129    |\n",
      "|    value_loss           | 76.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 247         |\n",
      "|    ep_rew_mean          | 276         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2152        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005087347 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 384         |\n",
      "|    policy_gradient_loss | 0.000793    |\n",
      "|    value_loss           | 77.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 243          |\n",
      "|    ep_rew_mean          | 277          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2152         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040307315 |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.664       |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.02         |\n",
      "|    n_updates            | 388          |\n",
      "|    policy_gradient_loss | 9.9e-05      |\n",
      "|    value_loss           | 47.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 233          |\n",
      "|    ep_rew_mean          | 270          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2153         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035357801 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.655       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34         |\n",
      "|    n_updates            | 392          |\n",
      "|    policy_gradient_loss | 0.00109      |\n",
      "|    value_loss           | 2.63         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 233          |\n",
      "|    ep_rew_mean          | 270          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2153         |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050231903 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.652       |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.26         |\n",
      "|    n_updates            | 396          |\n",
      "|    policy_gradient_loss | -0.000216    |\n",
      "|    value_loss           | 170          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 237          |\n",
      "|    ep_rew_mean          | 276          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2153         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 243          |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035917922 |\n",
      "|    clip_fraction        | 0.0441       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.954        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.8          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | 0.000455     |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 234         |\n",
      "|    ep_rew_mean          | 273         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2156        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 540672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004318809 |\n",
      "|    clip_fraction        | 0.051       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 311         |\n",
      "|    n_updates            | 404         |\n",
      "|    policy_gradient_loss | 0.000677    |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 236          |\n",
      "|    ep_rew_mean          | 275          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2159         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 257          |\n",
      "|    total_timesteps      | 557056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028911901 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.637       |\n",
      "|    explained_variance   | 0.957        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.7         |\n",
      "|    n_updates            | 408          |\n",
      "|    policy_gradient_loss | 0.000697     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 239          |\n",
      "|    ep_rew_mean          | 275          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2162         |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 265          |\n",
      "|    total_timesteps      | 573440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037914398 |\n",
      "|    clip_fraction        | 0.0543       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.638       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08         |\n",
      "|    n_updates            | 412          |\n",
      "|    policy_gradient_loss | 0.00074      |\n",
      "|    value_loss           | 3.53         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 238          |\n",
      "|    ep_rew_mean          | 270          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2165         |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 272          |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045371633 |\n",
      "|    clip_fraction        | 0.0581       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.639       |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 416          |\n",
      "|    policy_gradient_loss | -0.000678    |\n",
      "|    value_loss           | 71.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 235         |\n",
      "|    ep_rew_mean          | 275         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2164        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 606208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003034959 |\n",
      "|    clip_fraction        | 0.037       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.7        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | 6.98e-05    |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 233         |\n",
      "|    ep_rew_mean          | 276         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2165        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004028856 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 424         |\n",
      "|    policy_gradient_loss | 0.000117    |\n",
      "|    value_loss           | 94.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 232          |\n",
      "|    ep_rew_mean          | 280          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2170         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 638976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075145373 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.632       |\n",
      "|    explained_variance   | 0.952        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 145          |\n",
      "|    n_updates            | 428          |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    value_loss           | 138          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 230          |\n",
      "|    ep_rew_mean          | 281          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2171         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 301          |\n",
      "|    total_timesteps      | 655360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047145393 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.618       |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 432          |\n",
      "|    policy_gradient_loss | 0.000991     |\n",
      "|    value_loss           | 73           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 234         |\n",
      "|    ep_rew_mean          | 283         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2171        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004924569 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 436         |\n",
      "|    policy_gradient_loss | -0.00011    |\n",
      "|    value_loss           | 5.16        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 230        |\n",
      "|    ep_rew_mean          | 279        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2172       |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 316        |\n",
      "|    total_timesteps      | 688128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00471822 |\n",
      "|    clip_fraction        | 0.057      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.609     |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.712      |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | 0.000896   |\n",
      "|    value_loss           | 2.35       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 230         |\n",
      "|    ep_rew_mean          | 279         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2172        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 704512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002909233 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.7        |\n",
      "|    n_updates            | 444         |\n",
      "|    policy_gradient_loss | 0.00017     |\n",
      "|    value_loss           | 77.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 231          |\n",
      "|    ep_rew_mean          | 279          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2173         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 331          |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033500546 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.597       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 448          |\n",
      "|    policy_gradient_loss | 0.000184     |\n",
      "|    value_loss           | 34.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 241         |\n",
      "|    ep_rew_mean          | 280         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2175        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005055195 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 452         |\n",
      "|    policy_gradient_loss | -0.000711   |\n",
      "|    value_loss           | 18.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 236         |\n",
      "|    ep_rew_mean          | 279         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2174        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004020866 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.627      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.874       |\n",
      "|    n_updates            | 456         |\n",
      "|    policy_gradient_loss | -0.000116   |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 234        |\n",
      "|    ep_rew_mean          | 280        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2174       |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 354        |\n",
      "|    total_timesteps      | 770048     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00366007 |\n",
      "|    clip_fraction        | 0.0354     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.601     |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.83       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | 0.000649   |\n",
      "|    value_loss           | 77.2       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 231          |\n",
      "|    ep_rew_mean          | 278          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2177         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 361          |\n",
      "|    total_timesteps      | 786432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022750963 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.624       |\n",
      "|    explained_variance   | 0.956        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 464          |\n",
      "|    policy_gradient_loss | -0.000364    |\n",
      "|    value_loss           | 143          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 228          |\n",
      "|    ep_rew_mean          | 275          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2177         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 368          |\n",
      "|    total_timesteps      | 802816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035556257 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.607       |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 468          |\n",
      "|    policy_gradient_loss | 0.000191     |\n",
      "|    value_loss           | 66.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 229         |\n",
      "|    ep_rew_mean          | 282         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2177        |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005099968 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.621      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 151         |\n",
      "|    n_updates            | 472         |\n",
      "|    policy_gradient_loss | 0.00055     |\n",
      "|    value_loss           | 93          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 229         |\n",
      "|    ep_rew_mean          | 281         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2180        |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004341857 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 476         |\n",
      "|    policy_gradient_loss | 0.000551    |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 226         |\n",
      "|    ep_rew_mean          | 279         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2180        |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003705903 |\n",
      "|    clip_fraction        | 0.0483      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 235          |\n",
      "|    ep_rew_mean          | 279          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2180         |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 398          |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044006472 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.615       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.7          |\n",
      "|    n_updates            | 484          |\n",
      "|    policy_gradient_loss | -0.000109    |\n",
      "|    value_loss           | 3.6          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 225         |\n",
      "|    ep_rew_mean          | 279         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2182        |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 405         |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003259065 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 488         |\n",
      "|    policy_gradient_loss | -0.000416   |\n",
      "|    value_loss           | 16.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 226          |\n",
      "|    ep_rew_mean          | 280          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2184         |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 412          |\n",
      "|    total_timesteps      | 901120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050986283 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45         |\n",
      "|    n_updates            | 492          |\n",
      "|    policy_gradient_loss | 0.000998     |\n",
      "|    value_loss           | 2.45         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 224         |\n",
      "|    ep_rew_mean          | 282         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2186        |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 419         |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004054145 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.594       |\n",
      "|    n_updates            | 496         |\n",
      "|    policy_gradient_loss | -0.00025    |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 221         |\n",
      "|    ep_rew_mean          | 280         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2189        |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 933888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003688307 |\n",
      "|    clip_fraction        | 0.0427      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.000242   |\n",
      "|    value_loss           | 30          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 221         |\n",
      "|    ep_rew_mean          | 280         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2192        |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004602894 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.89        |\n",
      "|    n_updates            | 504         |\n",
      "|    policy_gradient_loss | 0.00125     |\n",
      "|    value_loss           | 5.54        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 221          |\n",
      "|    ep_rew_mean          | 275          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2194         |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 440          |\n",
      "|    total_timesteps      | 966656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031543784 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.9         |\n",
      "|    n_updates            | 508          |\n",
      "|    policy_gradient_loss | 0.000817     |\n",
      "|    value_loss           | 34.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 225          |\n",
      "|    ep_rew_mean          | 273          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2196         |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 447          |\n",
      "|    total_timesteps      | 983040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033939492 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.593       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.4         |\n",
      "|    n_updates            | 512          |\n",
      "|    policy_gradient_loss | 0.00124      |\n",
      "|    value_loss           | 17.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 227          |\n",
      "|    ep_rew_mean          | 282          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2197         |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 454          |\n",
      "|    total_timesteps      | 999424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030509215 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.581       |\n",
      "|    explained_variance   | 0.95         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.9         |\n",
      "|    n_updates            | 516          |\n",
      "|    policy_gradient_loss | 0.000527     |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 222         |\n",
      "|    ep_rew_mean          | 284         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2198        |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 461         |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004508637 |\n",
      "|    clip_fraction        | 0.0472      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.81        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    value_loss           | 5.81        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x7feef9249850>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the agent\n",
    "* Stable-Baselines3 provides a method called ```evaluate_policy```\n",
    "* You can find the documentation [here](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
    "\n",
    "When evaluating the agent, we create the evaluation environment, different from the training environment.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/anaconda3/envs/gym/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward = 285.37 +/- 16.61\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model.save(\"ppo-LunarLander-v2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training for 10 million steps I got a mean reward of ```285.37 +/- 16.61```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Publish the trained model on the Hub\n",
    "We can publish the train model to the hub with one line of code. Buy using ```package_to_hub``` you evalate, record a replay and generate a model for the agent which is then pushed to the hub.\n",
    "\n",
    "* You can showcase the work\n",
    "* Visualize the agent playing\n",
    "* Share with the community an agent others can use\n",
    "* Access a leaderboard to see how well the agen is performing compared to the classmates at [the leaderboard](https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To enable sharing the model you need to:\n",
    "1. Create an account at HF\n",
    "2. Sign in and then get the authentication token from HF website by:\n",
    "        *  Create a new token with write role\n",
    "        *  Copy the token\n",
    "        *  Run cell with the pasted token\n",
    "\n",
    "I can't use the notebook login, so I used ```huggingface-cli login``` instead and pasted the token into the commandline."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# !git config --global credential.helper store"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Push the trained agent to the Hub using ```package_to_hub``` function\n",
    "\n",
    "\n",
    "### Package to hub\n",
    "Needs:\n",
    "* ```model```: the trained model\n",
    "* ```model_name```: the name of the trained model that we defined in ```model_save```\n",
    "* ```model_architecture```: the model architecture we used (in our case PPO)\n",
    "* ```env_id```: the name of the environment, in our case ```LunarLander-v2```\n",
    "* ```eval_env```: the evaluation environment defined in ```eval_env```\n",
    "* ```repo_id```: the name of HF Hub Respository that will be created or updated (```repo_id = {username}/{repo_name}``` - a good name is {username}/{model_architecture}-{env-id}\n",
    "* ```commit message```: message of the commit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GIT large file storage\n",
    "Uploaded requires lfs:\n",
    "\n",
    "Download the package from [git-lfs](https://git-lfs.github.com/)\n",
    "\n",
    "Go to the download\n",
    "\n",
    "```\n",
    "tar -xf git-lfs-linux-amd64-v2.9.0.tar.gz\n",
    "\n",
    "chmod 755 install.sh\n",
    "\n",
    "sudo ./install.sh\n",
    "```\n",
    "\n",
    "Go to repository directory (gym)\n",
    "\n",
    "```\n",
    "git lfs install\n",
    "```\n",
    "You only need to do  this once"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uploaded in HF\n",
    "Note that for this notebook, it cannot start the video of the lander. But if you perform this in a python file it all works, then uploads the model to HF. Future tutorials should be done"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4m This function will save, evaluate, generate a video of your agent,\n",
      "create a model card and push everything to the hub. It might take up to 1min.\n",
      "This is a work in progress: If you encounter a bug, please open an issue and use\n",
      "push_to_hub instead.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/anaconda3/envs/gym/lib/python3.9/site-packages/huggingface_hub/hf_api.py:79: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.7. Pass `repo_id` instead.\n",
      "  warnings.warn(\n",
      "/home/wesley/PycharmProjects/deep-rl-class/unit1/hub/ppo-LunarLander-v2 is already a clone of https://huggingface.co/wesleywt/ppo-LunarLander-v2. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/home/wesley/anaconda3/envs/gym/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ContextException",
     "evalue": "Could not create GL context",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mContextException\u001B[0m                          Traceback (most recent call last)",
      "Input \u001B[0;32mIn [32]\u001B[0m, in \u001B[0;36m<cell line: 29>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     26\u001B[0m eval_env \u001B[38;5;241m=\u001B[39m DummyVecEnv([\u001B[38;5;28;01mlambda\u001B[39;00m : gym\u001B[38;5;241m.\u001B[39mmake(env_id)])\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Fill in the package_to_hub\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m \u001B[43mpackage_to_hub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m               \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m               \u001B[49m\u001B[43mmodel_architecture\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_architecture\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m               \u001B[49m\u001B[43menv_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m               \u001B[49m\u001B[43meval_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m               \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m               \u001B[49m\u001B[43mcommit_message\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_message\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/huggingface_sb3/push_to_hub.py:322\u001B[0m, in \u001B[0;36mpackage_to_hub\u001B[0;34m(model, model_name, model_architecture, env_id, eval_env, repo_id, commit_message, is_deterministic, n_eval_episodes, token, local_repo_path, video_length)\u001B[0m\n\u001B[1;32m    317\u001B[0m mean_reward, std_reward \u001B[38;5;241m=\u001B[39m _evaluate_agent(\n\u001B[1;32m    318\u001B[0m     model, eval_env, n_eval_episodes, is_deterministic, repo_local_path\n\u001B[1;32m    319\u001B[0m )\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m# Step 4: Generate a video\u001B[39;00m\n\u001B[0;32m--> 322\u001B[0m \u001B[43m_generate_replay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplay_env\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvideo_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_deterministic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrepo_local_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;66;03m# Step 5: Generate the model card\u001B[39;00m\n\u001B[1;32m    325\u001B[0m generated_model_card, metadata \u001B[38;5;241m=\u001B[39m _generate_model_card(\n\u001B[1;32m    326\u001B[0m     model_architecture, env_id, mean_reward, std_reward\n\u001B[1;32m    327\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/huggingface_sb3/push_to_hub.py:125\u001B[0m, in \u001B[0;36m_generate_replay\u001B[0;34m(model, eval_env, video_length, is_deterministic, repo_local_path)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;66;03m# Step 1: Create the VecVideoRecorder\u001B[39;00m\n\u001B[1;32m    117\u001B[0m env \u001B[38;5;241m=\u001B[39m VecVideoRecorder(\n\u001B[1;32m    118\u001B[0m     eval_env,\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \u001B[38;5;66;03m# Temporary video folder\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    122\u001B[0m     name_prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    123\u001B[0m )\n\u001B[0;32m--> 125\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:68\u001B[0m, in \u001B[0;36mVecVideoRecorder.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvObs:\n\u001B[1;32m     67\u001B[0m     obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvenv\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m---> 68\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_video_recorder\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obs\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:80\u001B[0m, in \u001B[0;36mVecVideoRecorder.start_video_recorder\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     75\u001B[0m base_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvideo_folder, video_name)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvideo_recorder \u001B[38;5;241m=\u001B[39m video_recorder\u001B[38;5;241m.\u001B[39mVideoRecorder(\n\u001B[1;32m     77\u001B[0m     env\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, base_path\u001B[38;5;241m=\u001B[39mbase_path, metadata\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_id}\n\u001B[1;32m     78\u001B[0m )\n\u001B[0;32m---> 80\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvideo_recorder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcapture_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecorded_frames \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecording \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:132\u001B[0m, in \u001B[0;36mVideoRecorder.capture_frame\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    129\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCapturing video frame: path=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath)\n\u001B[1;32m    131\u001B[0m render_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mansi\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mansi_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 132\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrender_mode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m frame \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_async:\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:85\u001B[0m, in \u001B[0;36mDummyVecEnv.render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;124;03mGym environment rendering. If there are multiple environments then\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m:param mode: The rendering type.\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrender(mode\u001B[38;5;241m=\u001B[39mmode)\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/gym/core.py:295\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, mode, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/gym/envs/box2d/lunar_lander.py:388\u001B[0m, in \u001B[0;36mLunarLander.render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 388\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menvs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassic_control\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rendering\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    391\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewer \u001B[38;5;241m=\u001B[39m rendering\u001B[38;5;241m.\u001B[39mViewer(VIEWPORT_W, VIEWPORT_H)\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py:27\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m    Cannot import pyglet.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     )\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyglet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m    Error occurred while running `from pyglet.gl import *`\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/gl/__init__.py:232\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_pyglet_doc_run \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyglet.window\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _sys\u001B[38;5;241m.\u001B[39mmodules \u001B[38;5;129;01mand\u001B[39;00m _pyglet\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshadow_window\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;66;03m# trickery is for circular import\u001B[39;00m\n\u001B[1;32m    231\u001B[0m     _pyglet\u001B[38;5;241m.\u001B[39mgl \u001B[38;5;241m=\u001B[39m _sys\u001B[38;5;241m.\u001B[39mmodules[\u001B[38;5;18m__name__\u001B[39m]\n\u001B[0;32m--> 232\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyglet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/window/__init__.py:1899\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m   1897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_pyglet_doc_run:\n\u001B[1;32m   1898\u001B[0m     pyglet\u001B[38;5;241m.\u001B[39mwindow \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules[\u001B[38;5;18m__name__\u001B[39m]\n\u001B[0;32m-> 1899\u001B[0m     \u001B[43mgl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_shadow_window\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/gl/__init__.py:206\u001B[0m, in \u001B[0;36m_create_shadow_window\u001B[0;34m()\u001B[0m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyglet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Window\n\u001B[0;32m--> 206\u001B[0m _shadow_window \u001B[38;5;241m=\u001B[39m \u001B[43mWindow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisible\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    207\u001B[0m _shadow_window\u001B[38;5;241m.\u001B[39mswitch_to()\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyglet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m app\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/window/xlib/__init__.py:171\u001B[0m, in \u001B[0;36mXlibWindow.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_handlers[message] \u001B[38;5;241m=\u001B[39m func\n\u001B[0;32m--> 171\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mXlibWindow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m _can_detect_autorepeat\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _can_detect_autorepeat \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/window/__init__.py:615\u001B[0m, in \u001B[0;36mBaseWindow.__init__\u001B[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001B[0m\n\u001B[1;32m    612\u001B[0m     config \u001B[38;5;241m=\u001B[39m screen\u001B[38;5;241m.\u001B[39mget_best_config(config)\n\u001B[1;32m    614\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m context:\n\u001B[0;32m--> 615\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_context\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;66;03m# Set these in reverse order to above, to ensure we get user preference\u001B[39;00m\n\u001B[1;32m    618\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_context \u001B[38;5;241m=\u001B[39m context\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/gl/xlib.py:204\u001B[0m, in \u001B[0;36mXlibCanvasConfig13.create_context\u001B[0;34m(self, share)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_context\u001B[39m(\u001B[38;5;28mself\u001B[39m, share):\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglx_info\u001B[38;5;241m.\u001B[39mhave_extension(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGLX_ARB_create_context\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 204\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mXlibContextARB\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshare\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m XlibContext13(\u001B[38;5;28mself\u001B[39m, share)\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/gl/xlib.py:314\u001B[0m, in \u001B[0;36mXlibContext13.__init__\u001B[0;34m(self, config, share)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config, share):\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mXlibContext13\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshare\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglx_window \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/pyglet/gl/xlib.py:218\u001B[0m, in \u001B[0;36mBaseXlibContext.__init__\u001B[0;34m(self, config, share)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglx_context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_glx_context(share)\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglx_context:\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# TODO: Check Xlib error generated\u001B[39;00m\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m gl\u001B[38;5;241m.\u001B[39mContextException(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCould not create GL context\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_have_SGI_video_sync \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mglx_info\u001B[38;5;241m.\u001B[39mhave_extension(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGLX_SGI_video_sync\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_have_SGI_swap_control \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mglx_info\u001B[38;5;241m.\u001B[39mhave_extension(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGLX_SGI_swap_control\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mContextException\u001B[0m: Could not create GL context"
     ]
    }
   ],
   "source": [
    "# HF imports\n",
    "import gym\n",
    "from huggingface_sb3 import package_to_hub\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# Define the name of the trained model that we defined in model_save\n",
    "model_name = \"ppo-LunarLander-v2_test\"\n",
    "\n",
    "# Define the model architecture\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "# repo_id is the id of the model repository from HF hub (repo_id = {wesleywt}/{repo_name}\n",
    "\n",
    "repo_id = \"wesleywt/ppo-LunarLander-v2\"\n",
    "\n",
    "# Commit message\n",
    "commit_message = \"First commit of model\"\n",
    "\n",
    "\n",
    "# Create evaluation env\n",
    "eval_env = DummyVecEnv([lambda : gym.make(env_id)])\n",
    "\n",
    "# Fill in the package_to_hub\n",
    "package_to_hub(model=model,\n",
    "               model_name=model_name,\n",
    "               model_architecture=model_architecture,\n",
    "               env_id=env_id,\n",
    "               eval_env=eval_env,\n",
    "               repo_id=repo_id,\n",
    "               commit_message=commit_message)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have trained and uploaded the Deep RL Learning agent. But it will not upload because jupyter cannot start the video of the training.\n",
    "\n",
    "Maybe use Jupyter Labs?\n",
    "\n",
    "It works in a normal Python script: ```train_upload_lunar_lander_v2.py```\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Additional Challengers\n",
    "We can optimize the hyperparameters to get better training results. For example increasing the number of training steps.\n",
    "\n",
    "Some ideas:\n",
    "1. Train more steps\n",
    "2. Hyperparameter optimization. You can find the parameters [here](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters)\n",
    "3. Try other model architectures such as DQN. You can find them [here]((https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html))\n",
    "4. Push and compare the results on the [leaderboard](https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard)\n",
    "\n",
    "## Other environments\n",
    "Try other environments such as CartPole-V1 or MountainCar-v0 etc. Check out how they work [here](https://www.gymlibrary.ml/\n",
    "\n",
    "## Weights and Bias\n",
    "I am thinking of using W&B for hyperparameter optimizationss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}